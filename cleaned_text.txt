 DAVE: Security attacks are on the rise, and new vulnerabilities are exploited and discovered every week. No matter how prepared an organization may be in the event of a security attack, at some point, something goes wrong. Whether it's a data breach, ransomware, or a simple mistake made by an employee, incidents happen. And it's up to security professionals like you to effectively respond to security incidents. Hello, and welcome to the course. I'm Dave, and I'm a principal security strategist for Google Cloud. I have 20 years of experience as a security practitioner and leader. Over the past eight years, I've worked at industry-leading security vendors like Fortinet, Splunk, and Google, where I developed a specialty in security analytics. I have a passion for helping analysts develop the skills necessary to succeed in their careers. I'm so happy you're here. You've done a great job so far. You've learned a lot about security concepts, best practices, and types of security attacks. Now, in this course, we'll focus on incident detection, analysis, and response. You'll have the opportunity to apply your learning using tools such as tcpdump, Wireshark, Suricata, Splunk, and Chronicle. By the end of this course, you'll have an in-depth understanding of incident response. First, you'll learn about the incident response lifecycle and how incident response teams work together. You'll also learn about the types of tools used in detection and response, including documentation. You'll also be given your own Incident Handler's Journal that you'll use during your investigations. Next, you'll apply your knowledge and networking in Linux to monitor and analyze network traffic using packet sniffers like Wireshark and tcpdump to capture and analyze packets for potential indicators of security incidents. Then you'll become familiar with the common processes and procedures used during incident detection and response. You'll learn how to use investigative tools to analyze and verify incidents and produce documentation. Finally, you'll learn how to interpret logs and alerts. You'll learn how detection tools produce logs and how these logs are analyzed in security information and event management tools. Ready to begin? Let's get started. Welcome. In my role, as principal security strategist, I've seen how the incident response operations that you'll learn about in this course are implemented in an organization. One of the things I find so exciting about detecting and responding to incidents is the challenge of using data to understand what an adversary has done in my organization's environment. No two investigations are ever the same, but there are patterns of behavior that you can learn to spot as you hone your analytic skills. Previously, you established a solid understanding of asset security, threats, and vulnerabilities. You explored the NIST cybersecurity framework or CSF as a methodology for risk management. You learned about mitigating organizational risk through classifying and securing assets. And you also explored security and privacy controls to safeguard data. You used tools like MITRE and CVE to investigate common vulnerabilities and use techniques like threat modeling to develop an attacker's mindset. Next, we'll revisit the NIST CSF with a focus on the incident response lifecycle. You'll be given your own Incident Handler's journal which you'll use throughout the rest of the course. You'll also be introduced to incident response teams, including the different team roles and how they organize to respond to incidents. And finally, you'll learn about the different types of documentation, detection, and management tools you'll use as a security professional working in incident response. Later on, you'll have an opportunity to use these tools. Are you ready to begin your journey in detection and response? Let's begin. Incident lifecycle frameworks provide a structure to support incident response operations. Frameworks help organizations develop a standardized approach to their incident response process so that incidents are managed in an effective and consistent way. There are many different types of frameworks that organizations can adopt and modify according to their needs. In this course, we'll focus on the NIST CSF. Then we'll expand on the CSF and discuss the phases of a NIST incident response lifecycle. To recall, the five core functions of the NIST CSF are identify, protect, detect, respond, and recover. This course, we'll explore the last three steps of this framework-- detect, respond, and recover. These last three steps are critical stages during incident response. And as an analyst, you'll detect and respond to incidents and implement actions for recovery. The NIST incident response lifecycle is another NIST framework with additional substeps dedicated to incident response. It begins with preparation, next, detection and analysis, and then containment, eradication, and recovery, and, finally, post-incident activity. One thing to note is that the incident lifecycle isn't a linear process. It's a cycle, which means that steps can overlap as new discoveries are made. This lifecycle gives us a blueprint of how to effectively respond to incidents. But before we dive into incident detection and response, let's take some time to understand what an incident is. According to NIST, an incident is an occurrence that actually or imminently jeopardizes, without lawful authority, the confidentiality, integrity, or availability of information or an information system or constitutes a violation or imminent threat of violation of law, security policies, security procedures, or acceptable-use policies. Whoa, that's a lot to take in. Let's break it down. It's important to understand that all security incidents are events, but not all events are security incidents. What are events? An event is an observable occurrence on a network, system, or device. Here's an example of an event. A user attempts to log in to their email account, but they can't, because they forgot their password. The user then requests a password reset and successfully changes their password. This is an observable event. Why? Because systems and applications log password reset requests, and logs provide evidence that something happened. We know that someone successfully requested a password reset and that they did not violate security policies to access the account. Now, imagine that, instead of the rightful owner of the account, a malicious actor trying to gain access to the account successfully initiated the password change request and changed the account password. This would be considered both an event and a security incident. It's an event because it's an observable occurrence. It's also a security incident because a malicious actor violated the security policy to unlawfully access an account that is not rightfully theirs. Remember, all security incidents are events, but not all events are security incidents. Just like detectives working a case carefully handle and document their evidence and findings, security analysts are required to do the same when they investigate a security incident. An incident investigation reveals critical information about the five Ws of an incident. Who triggered the incident? What happened? When the incident took place? Where the incident took place? And why the incident occurred? Keeping track of this information is essential, not only during an incident investigation, but also during the closure of an investigation when it comes time to write the final report. As an analyst, you'll need a method to document and reference this information for easy access when you need it. A great way to do this is to use an Incident Handler's Journal which is a form of documentation used in incident response. Throughout this course, you'll be using your own Incident Handler's Journal to take notes of any incident details. We'll discuss more on documentation in the upcoming lessons. Hi, again. In this section, we'll discuss how incident response teams manage incidents. You may have been part of a team before. Whether it was a sports team or a team in the workplace or at school, teams are most successful when everyone uses their diverse strengths to work towards a common goal. Incident response teams aren't any different. A successful response to security incidents doesn't happen in isolation. It requires a team of both security and non-security professionals working together with defined roles. Computer Security Incident Response Teams, or CSIRTs, are a specialized group of security professionals that are trained in incident management and response. The goal of CSIRTs are to effectively and efficiently manage incidents, provide services and resources for response and recovery, and prevent future incidents from occurring. Security is a shared responsibility, which is why CSIRTs certs must work cross-functionally with other departments to share relevant information. For example, if an incident resulted in the breach of sensitive data, like financial documents or PII, then the legal team must be consulted. Some regulatory compliance measures may require organizations to publicly disclose a security incident within a certain time frame. This means that CSIRTs must collaborate with the organization's public relations team to coordinate efforts for public disclosure. So how exactly does a CSIRT function? First, there's the security analyst. The analyst's job is to investigate security alerts to determine if an incident has occurred. If an incident has been detected, the analyst will determine the criticality rating of the incident. Some incidents can be easily remediated by the security analyst and don't require escalation. But if the incident is highly critical, it gets escalated to the technical lead who provides technical leadership by guiding security incidents through their lifecycle. During this time, the incident coordinator tracks and manages the activities of the CSIRTs and other teams involved in the response effort. Their job is to ensure that incident response processes are followed and that teams are regularly updated on the incident status. Not all CSIRTs are the same. Depending on the organization, a CSIRTs can also be referred to as an Incident Handling Team, or IHT, or Security Incident Response Team, SIRT. Depending on an organization's structure, some teams can also have a broader or specialized focus. For example, some teams may be solely dedicated to crisis management, and others may be incorporated with a SOC. Roles can have different names too. For example, a technical lead can also be known as an ops lead. Regardless of the team's title or focus, they all share the same goal, incident management and response. Now that a bit about incident response teams, we'll continue to learn about how incident response teams plan, organize, and respond to incidents. I'll meet you in the next video. So you've learned about incident response teams, the different types of roles, and their respective responsibilities. Now let's talk about how teams respond to incidents using incident response plans. When an incident occurs, incident response teams must be prepared to respond quickly, efficiently, and effectively. Whether it's a data breach, DDoS attack, or ransomware, incidents have the potential to cause significant damage to an organization. Like we previously mentioned, regulations may require organizations to report incidents within a certain time frame. So it's crucial for organizations to have a formal incident response plan in place so there's a prepared and consistent process to quickly respond to incidents once they occur. You may remember learning that security plans consist of three basic elements-- policies, standards, and procedures. An incident response plan is a document that outlines the procedures to take in each step of incident response. Response plans, just like response teams, are not all the same. Organizations tailor their plans to meet their unique requirements, such as their mission, size, culture, industry, and structure. For example, smaller organizations may choose to include their incident response plan in their security plan, while others may choose to have them as separate documents. Although not all incident plans are the same, there are common elements that they share. Incident plans have incident response procedures. These are step-by-step instructions on how to respond to incidents. System information-- these are things like network diagrams, data flow diagrams, logging, and asset inventory information, and other documents like contact lists, forms, and templates. Plans aren't perfect, and there's always room to adjust and improve as incidents occur. Incident processes and procedures must be regularly reviewed and tested. This can be done through exercises like tabletops or simulations. These exercises ensure that all team members are familiar with the response plan. They also allow organizations to identify any missing gaps in a process to improve their incident response plan. Also, organizations may be required to complete specific types of exercises for regulatory reasons. Coming up, we'll discuss the different types of tools used in incident response. As a security analyst, you'll play an important role in incident detection. After all, you're going to be at the front lines actively detecting threats. To do this, you'll not only rely on the security knowledge you've developed so far, but you'll also be using a variety of tools and technologies to support your investigations. A great carpenter doesn't just use a hammer to create a piece of furniture. They rely on a variety of tools in their toolbox to get the job done. They'll need to use a tape measure to measure dimensions, a saw to cut wood, and sandpaper to smooth the surface. Likewise, as a security analyst, you won't be using a single tool to monitor, detect, and analyze events. You'll use detection and management tools to monitor system activity to identify events that require investigation. You'll use documentation tools to collect and compile evidence, and you'll also use different investigative tools for analyzing these events, like packet sniffers. New security technologies emerge, threats evolve, and attackers become stealthier to avoid detection. To become effective at detecting threats, you'll need to continuously expand your security toolbox. That's what makes the security field such an exciting one to be in. There's always something new to be learned. You might remember the Incident Handler's Journal we shared with you from the previous section. You'll be using this journal as your own form of documentation as you work through the rest of this course. Consider this to be your first security tool to add to your toolbox. Hi, there. Previously you learned how an Incident Handler's Journal is used for documenting the five Ws of an incident-- who, what, where, when, and why an incident occurred. In this section, we'll continue our discussion on documentation by exploring the different types of documentation, the importance of effective documentation, and we'll finish off with a discussion on documentation tools. Documentation is any form of recorded content that is used for a specific purpose. This can be audio, digital, or handwritten instructions and even videos. There is no set industry standard for documentation. So many organizations set their own documentation practices. Regardless, documentation is meant to provide instruction and guidance on a specific topic. There are also many types of documentation. And you may already be familiar with some of them from the previous lessons. These include playbooks, incident handlers' journals, policies, plans, and final reports. Remember, there isn't an industry standard for documentation, which means that one organization's documentation practices may look completely different than another's. Often, organizations will tailor their documentation practices according to their needs and legal requirements. They may add, remove, or even merge documentation types. Have you ever purchased a product and didn't know how to use it and consulted the product manual for instructions on how to do something like turn it on? Congrats. You've used documentation to solve an issue. Previously, you've learned about how playbooks keep business operations safe. And in incident response, playbooks work similar to a product manual. As a refresher, a playbook is a manual that provides details about any operational action. You'll learn more about playbooks later. Let's revisit that product manual example. Have you ever consulted a product manual for help and found yourself confused with the instructions and unable to get the help you needed? Whether it's had to do with unclear visuals and instructions or a confusing layout, you weren't able to use the documentation to solve your issue. This is an example of ineffective documentation. Effective documentation reduces uncertainty and confusion. This is critical during a security incident when tensions are high and urgent response is required. As a security professional, you'll be using and creating documentation regularly. It's essential that the documentation you use and produce is clear, consistent, and accurate so that you and your team can respond swiftly and decisively. Word processors are a common way to document. Some popular tools to use our Google Docs, OneNote, Evernote, and Notepad++. Ticketing systems like Jira can also be used to document and track incidents. Lastly, Google Sheets, audio recorders, cameras, and handwritten notes are also tools you can use to document. Our discussion on documentation has only just begun. Soon, you'll use your Incident Handler's Journal to put your documentation skills to work. In this video, we'll introduce you to intrusion detection and intrusion prevention systems. Imagine that you have just installed a home intrusion security system. You've installed intruder sensors for each entry and exit point in your home, including doors and windows. Those sensors work by sending out sound waves. And when an object touches a sound wave, the waves bounce back to your sensor and trigger an alert to your phone, notifying you that an intrusion was detected. And Intrusion Detection System, or IDS, works in a very similar way to home intrusion sensors. An intrusion detection system is an application that monitors system and network activity and produces alerts on possible intrusions. Like the home intrusion sensor, IDS collect and analyze system information for abnormal activities. If something unusual is detected, the IDS sends out an alert to appropriate channels and personnel. Now, imagine a jewelry storefront with a window sensor. When the sensor detects that the window's glass has been shattered, it triggers a steel roll up door to automatically replace the shattered window and prevent unauthorized entry into the store. This is what an intrusion prevention system does. Intrusion Prevention Systems, or IPS, have all the same capabilities as an IDS, but they can do more. They monitor system activity for intrusions and take action to stop it. Many tools have the ability to perform the function of both IDS and IPS. Some popular tools are Snort, Zeek, Kismet, Sagan, and Suricata. We will be exploring Suricata in upcoming lessons. You might be wondering, where do these alert notifications go? Well, coming up, we'll discuss how to manage alerts using security information and event management tools. Our discussion on detection tools may have left you wondering where alerts are sent and how alerts are accessed by security analysts? This is where Security Information and Event Management, or SIEM tools, are used. SIEM is a tool that collects and analyzes log data to monitor critical activities in an organization. SIEM provides security professionals with a high level overview of what goes on in their networks. How exactly does it do this? Let's use an example of a car. Cars have many different parts-- tires, lights, and let's not forget all the internal machinery that's under the hood. There are many different components of a car. But how do if one of them has an issue? Aha, you guessed it, the dashboard warning lights. The dashboard notifies you about information related to the car's components-- whether the tire pressure or battery voltage is low, you need to refuel, or a door hasn't been properly closed. A car's dashboard notifies you about the status of the car's components so that you can take action to fix it. SIEM tools work in a similar way. Just like cars have many different components, a network can have thousands of different devices and systems which make monitoring them quite the challenge. A car's dashboard gives the driver a clear picture of the status of their car so they don't have to worry about inspecting each component themselves. Similarly, a SIEM looks at data flows between all the different systems in a network and analyzes them to provide a real-time picture of any potential threats to the network. It does this by ingesting massive amounts of data and categorizes this data so that it's easily accessible through a centralized platform similar to a car's dashboard. Here's what the process looks like. First, SIEM tools collect and aggregate data. This data is typically in the form of logs, which are basically a record of all the events that happened on a given source. Data can come from multiple sources, such as IDS or IPS, databases, firewalls, applications, and more. After all this data gets collected, it gets aggregated. Aggregation simply means all this data from different data sources get centralized in one place. Depending on the number of data sources a SIEM collects from, a huge volume of raw, unedited data can get collected. And not all data that's collected by a SIEM is relevant for security analysis purposes. Next, SIEM tools normalize data. Normalization takes the raw data that the SIEM has collected and cleans it up by removing non-essential attributes so that only what's relevant is included. Data normalization also creates consistency in log records which is helpful when you're searching for specific log information during incident investigation. Finally, the denormalized data gets analyzed according to configured rules. SIEM analyzes the denormalized data against a rule set to detect any possible security incidents which then get categorized or reported as alerts for security analysts to review. Now that you've explored the capabilities of SIEM tools, let's examine another security management tool, Security Orchestration Automation and Response, or SOAR, is a collection of applications, tools, and workflows that uses automation to respond to security events. While SIEM tools collect, analyze, and report on security events for security analysts to review, SOAR automates analysis and response to security events and incidents. SOAR can also be used to track and manage cases. Multiple incidents can form a case. And SOAR offers a way to view all of these incidents in one central place. Well, there you have it. You've learned how incident management tools like SIEM and SOAR make it easier for security analysts to see what's happening in a network and to respond to any threats efficiently. Way to go. You made it through a new section, and you've learned a lot. As a refresher, we first covered the incident response lifecycle as a framework to support incident response processes. You were also given your very own Incident Handler's Journal for your incident investigations, which you'll continue to use throughout this course. You explored how incident response teams operate together to respond to incidents using incident plans. You also learned about the documentation, detection, and management tools used during incident response. Congrats on making it through the first part of your incident response journey. Coming up, we'll explore network monitoring. You'll also have the opportunity to apply your learning through the activities. I'll meet you in the next section. Welcome back. I'm so glad you're joining us. Previously, you were introduced to incident detection and response. You may also remember learning about networking from a previous course. To recap, you learned about how devices talk to each other using network protocols and the different types of network attacks. You also examined some network security best practices. Here, we'll expand on networking and shift our focus to network analysis. First, you'll examine network communications by exploring network traffic flows. Next, you'll learn about viewing and capturing network traffic using packet sniffers. Then you'll be introduced to packet analysis where you'll examine packet fields and decode communication between devices and networks. As a security professional, you'll be tasked with monitoring networks and system infrastructure to detect malicious activities. And this section will provide you with the opportunity to develop your network and packet analysis skills. Are you ready to begin? Let's get started. In many organizations, network communication travels over multiple networks in different countries and across different devices. Data can get unintentionally sent and stored in insecure places like personal email inboxes or cloud storage platforms. Users trust that their data is safely and securely sent and stored. And it's the job of security professionals like you to help protect these communications in transit and at rest. Previously, you may recall learning how to identify and secure critical assets through security controls like data classification and encryption. Coming up, we'll expand on this topic and examine how network traffic analysis can be used to monitor network activity and identify potential malicious activity. So what is network traffic? Network traffic is the amount of data that moves across a network, while network data is the data that's transmitted between devices on a network. Depending on the size of a network, there can be a huge volume of network traffic at any given moment. For example, in a large multinational organization, there may be thousands of employees sending and receiving emails at any given time. That's a lot of network traffic. With such large volumes of traffic being produced, how do you know what's normal behavior or what's unusual and requires investigation as a potential security incident? Imagine being stuck in unexpected traffic during your regular drive to work. And as you move along, you realize something unusual caused the traffic, like a minor vehicle collision which slowed down the expected flow. On the road, we have certain expectations about traffic flows based on our commuting experience. Peak traffic patterns like morning and evening rush are normal and expected, while abnormal traffic during off-peak times reveals that something unexpected has happened like a vehicle collision. Network traffic works in the same way. By understanding how data should be flowing across the network, you can develop an understanding of expected network traffic flow. By knowing what's normal, you can easily spot what's abnormal. We can detect traffic abnormalities through observation to spot Indicators Of Compromise, also known as IOC, which are observable evidence that suggests signs of a potential security incident. Take, for instance, data exfiltration, which is the unauthorized transmission of data from a system. Attackers use data exfiltration to steal or leak data, such as usernames, passwords, or intellectual property. By observing network traffic, we can determine if there's any indicators of compromise, such as large volumes of outbound traffic leaving a host. This is a sign of possible data exfiltration which can be further investigated. Understanding and monitoring network traffic for inconsistencies is an important aspect of a security professional's job. Coming up, we'll explore what a data exfiltration attack looks like in real time. Meet you there. Monitoring network traffic helps security professionals detect, prevent, and respond to attacks. In my experience as a security professional, monitoring for deviations from typical network traffic patterns has yielded big results. Even if information is encrypted, monitoring network traffic is still important for security purposes. Let's discuss how the detection and response process might work in a data exfiltration attack. First, we'll outline the attackers perspective. Before attackers can perform data exfiltration, they'll need to gain initial access into a network and computer system. This can be done through a social engineering attack like phishing, which tricks people into disclosing sensitive data. Attackers can send phishing emails with attachments or links that trick their target into entering their credentials. Now an attacker has successfully gained access to their device. After gaining their initial position into the system, an attacker won't stop there. The goal for attackers is to maintain access in the environment and avoid being detected for as long as possible. To do this, they'll perform a tactic known as lateral movement or pivoting. This is when they'll spend time exploring the network with the goal of expanding and maintaining their access to other systems on the network. As an attacker pivots in the network, they'll scope out the environment to identify valuable assets, such as sensitive data like proprietary code, personally identifiable information like names and addresses, or financial records. They'll do this by searching locations such as network file shares, intranet sites, code repositories, and more. After the attacker identifies the assets of value, they'll need to collect, package, and prepare the data for exfiltration outside of the organization's network and into the attacker's hands. One way they may do this is by reducing the data size. This helps attackers hide the stolen data and bypass security controls. Finally, the attacker will exfiltrate the data to their destination of choice. There are many ways to do this. For example, attackers can email the stolen data to themselves using the compromised email account. Now that you've tapped into the attacker's perspective, let's explore how organizations can defend against this type of attack. First, security teams must prevent attacker access. There are many methods you can use to protect your network from phishing attempts, for example, requiring users to use multifactor authentication. Attackers that gain access to a network can remain unnoticed for a while. It's important that security teams monitor network activity to identify any suspicious activity that can indicate a compromise. for example, multiple user logins coming from IP addresses outside of the network should be investigated. Earlier, you examined how to identify, classify, and protect assets using asset inventories and security controls. As part of an organization's security policy, all assets should be cataloged in an asset inventory. The appropriate security controls should also be applied to protect these assets from unauthorized access. Lastly, if a data exfiltration attack is successful, security teams must detect and stop the exfiltration. To detect the attack, indicators of unusual data collection can be identified through network monitoring. These include large internal file transfers, large external uploads, and unexpected file rights. SIEM tools can detect and alert on these activities. Once an alert has been sent out, security teams investigate and stop the attack from continuing. There are many ways to stop an attack like this. For instance, once the unusual activity is identified, you can block the IP addresses associated with the attacker using firewall rules. Data exfiltration attacks are just one of many attacks that can be detected through network monitoring. Coming up, you'll learn how to monitor and analyze network communications using packet sniffers. Whether it's an employee sending an email or a malicious actor attempting to exfiltrate confidential data, actions that are performed on a network can be identified through examining network traffic flows. Understanding these network communications provides valuable insight into the activities happening in a network. This way, you can better understand what's going on in an environment and defend against potential threats. With this in mind, let's examine how to record network traffic through packet captures. Previously in the program, you learn that, when data is sent, it's divided into packets. Just like an addressed envelope in the mail, packets contain delivery information which is used to route it to its destination. This information includes a sender and receiver's IP address, the type of packet that's being sent, and more. Packets can provide lots of information about the communications happening between devices over a network. You may also recall that a packet has multiple components. There's the header which includes information like the type of network protocol and port being used. Imagine this as being the name and mailing address located on an envelope. Network protocols are a set of rules that determine the transmission of data between devices on a network. Ports are non-physical locations on a computer that organize data transmission between devices on a network. The header also contains the packet's source and destination IP address. We'll explore more information contained in the header in a later section. Next, there's the payload which contains the actual data that's being delivered. This is like the content of a letter inside of an envelope. And there's the footer, which signifies the end of a packet. So how exactly can you observe a network packet? Just like scents are invisible but can be smelled, packets are invisible but can be captured using tools called packet sniffers. You may remember packet sniffers from a previous section. A network protocol analyser or packet sniffer is a tool designed to capture and analyze data traffic within a network. As a security analyst, you'll use packet sniffers to inspect packets for indicators of compromise. Through packet sniffing, we can grab a detailed snapshot of packets that travel over a network in the form of a packet capture. A Packet Capture or P-cap is a file containing data packets intercepted from an interface or network. It's sort of intercepting an envelope in the mail. Packet captures are incredibly useful during incident investigation. By having access to the communications happening between devices over a network, you can observe network interactions and start to build a storyline to determine what exactly happened. Coming up, we'll discuss the importance of packet analysis. Meet you there. If a packet capture is like intercepting an envelope in the mail, then packet analysis is like reading the letter inside of the envelope. Let's discuss how analyzing packets can help us interpret and understand network communications. As you may know, networks are noisy. There's an enormous volume of communications happening between devices at any given time. And because of this, packet captures can contain large amounts of network communications, making analysis challenging and time-consuming. As a security professional, you'll be working against the clock to protect networks and computer systems from potential attacks. You may analyze network evidence in the form of packet captures to identify indicators of compromise. Having the ability to filter network traffic using packet sniffers to gather relevant information is an essential skill to have. For example, let's say that you were tasked with analyzing a packet capture to find any indication of data exfiltration. How would you go about this? Using a network analyzer tool, you can filter the packet capture to sort packets. This can help you quickly identify an event associated with data exfiltration like large amounts of data leaving a database. There are many other filters you can apply to packet captures to find the information you need to support an investigation efficiently. Examples of network analyser tools include tcpdump and Wireshark. Tcpdump is accessed through a command line, while Wireshark has a Graphical User Interface, or GUI. Both tools are useful for security analysts, and soon you'll have the opportunity to explore both. Before we begin using these tools, let's explore packet fields in detail, specifically IP headers. Meet you there. While there are many different tools available to use, it's important as a security analyst that you learn how to read and analyze packets manually. To do so, let's examine an important packet component, IP headers. Previously, you learned about the four layers of the TCP/IP model. Remember, the TCP/IP model is a framework that is used to visualize how data is organized and transmitted across a network. The internet layer accepts and delivers packets for the network. It's also the layer where the internet protocol operates as the foundation for all communications on the internet. It's responsible for making sure packets reach their destinations. The internet protocol operates like a mail courier delivering an envelope. Instead of using the delivery information found on the envelope, the internet protocol uses the information found in a packet header, like IP addresses. It then determines the best available route for packets to take so that data can be sent and received between hosts. As you may already know, IP packets contain headers. Headers contain the data fields essential to the transfer of data to its intended destination. Different protocols use different headers. There are two different versions of the internet protocol, IPv4, which is considered to be the foundation of internet communications, and IPv6, which is the most recent version of the internet protocol. Remember, different protocols use different headers. So IPv4 and IPv6 headers differ, but they contain similar fields with different names. IPv4 is still the most widely used, so we'll focus on examining the fields of an IPv4 header. Let's start with the version field, which specifies which version of IP is being used, either IPv4 or IPv6. Referring back to our mail analogy, the version field is like the different classes of mail, like priority, express, or regular. Next, IHL stands for, Internet Header Length. This field specifies the length of the IP header plus any options. The next field, TOS stands for Type Of Service. This field tells us if certain packets should be treated with different care. For example, think of TOS like a fragile sticker on a mail package. Next is the total length field, which identifies the length of the entire packet, including the headers and the data. This can be compared to the dimensions and weight of an envelope. The next three fields-- identification, flags, and fragment offset-- deal with information related to fragmentation. Fragmentation is when an IP packet gets broken up into chunks, which then get transmitted over the wire and reassembled when they arrive at their destination. These three fields specify if fragmentation has been used and how to reassemble the broken packets in the correct order. This is similar to how mail can travel through multiple routes, like mailboxes, processing facilities, airplanes, and mail trucks before it reaches its destination. The TTL field stands for Time To Live. Like its name suggests, this field determines how long a packet can live before it gets dropped. Without this field, packets could loop through routers endlessly. TTL is similar to how tracking information provides details about an envelope's expected delivery date. The protocol field specifies the protocol used by providing a value, which corresponds to a protocol. For example, TCP is represented by 6. This is similar to including the number of a house in a postal address. The header checksum stores a value called a checksum which is used to determine if any errors have occurred in the header. The source address specifies the source IP address, and the destination address specifies the destination IP address. This is just like the sender and receiver's contact information found on an envelope. The options field is not required and is commonly used for network troubleshooting rather than common traffic. If it's used, the header length increases. It's like purchasing postal insurance for an envelope. Finally, at the end of the packet header is where the packet's data resides, like the text in an email message. Who knew that the packets of data we send across networks contain so much information? Coming up soon, you'll have the opportunity to examine these packet fields in detail. Tcpdump is a popular network analyzer. It's preinstalled on many Linux distributions and can be installed on most Unix-like operating systems like macOS. You can easily capture and monitor network traffic, such as TCP, IP, ICMP, and many more. Tcpdump is a command line tool. This means that it does not have a graphical user interface. Earlier in the program, you learned that the command line is a very powerful and efficient tool. And we'll practice using it together. With tcpdump, you can apply options and flags to your commands to easily filter network traffic so that you can find exactly what you're looking for. You can filter for a specific IP address, protocol, or port number. Let's examine a simple tcpdump command use the capture packets. Keep in mind that your computer's traffic may appear different when you use this command. At first glance, this looks like a lot of information. Let's examine it line by line. The command we ran is sudo tcpdump -i any -v -c 1. We're using sudo because the Linux account we're logged in on doesn't have the permission to run tcpdump. Then we specify tcpdump to start tcpdump and -i to specify which interface we want to sniff traffic on. The -v stands for verbose which displays detailed packet information. The -c stands for count which specifies how many packets tcpdump will capture. Here, we've specified 1. Now let's examine the output. Tcpdump has told us that it's listening on any available network interfaces, and it's also given us additional information like the capture size. The first field is the packet's timestamp which details the specific time of the packet travel. It begins with hours, minutes, seconds, and fractions of a second. Timestamps are especially helpful during an incident investigation when you want to determine timelines and correlate traffic. Next, IP is listed as the version field. It's listed as IP, which means its IPv4. The verbose option has given us more details about the IP packet field, such as protocol type and the length of the packet. Let's check it out. The first field, tos, stands for type of service. Recall that this tells us if certain packets should be treated with different care. This is represented by a value in hexadecimal. The ttl field is time to live, which tells us how long a packet can travel across a network before it gets dropped. The next three fields are identification, offset, and flags which provide three fields with information relating to fragmentation. These fields provide instructions on how to reassemble packets in the correct order. For example, the DF beside flags stands for Don't Fragment. Next, the proto is the protocol field. It specifies the protocol in use and also provides us with a value that corresponds to the protocol. Here, the protocol is TCP, which is represented by the number 6. The last field, length, is the total length of the packet including the IP header. Next, we can observe the IP addresses that are communicating with each other. The direction of the arrow indicates the direction of the traffic flow. The last piece of the IP address indicates the port number or name. Next, the cksum or check sum field corresponds to the header checksum which stores a value that's used to determine if any errors have occurred in the header. Here, it's telling us it's correct with no errors. The remaining fields are related to TCP. For example, flags indicate TCP flags. The P is the push flag, and the period indicates it's an ACK flag. This means that the packet is pushing out data. This is just one of many commands you can use in tcpdump to capture network traffic. Isn't it fascinating to observe all the information contained within these invisible packets? Go ahead, and try it out for yourself. Nice work so far. Congratulations on capturing and analyzing your first packet. Let's review what we've covered so far. First, you learned how network traffic flows provide valuable communications insight. Through monitoring network activity for indicators of compromise, you learn how to spot unusual network activity like data exfiltration. Then you learned how to view and capture network traffic using packet sniffers. Finally, you learn how to inspect packets through packet analysis. You dissected packet header data fields and analyzed packet captures in detail. You've made a lot of progress in developing the skills you'll need to prepare for an entry-level role in security. Coming up, you'll be immersed into the exciting world of incident investigation where you'll examine the processes behind detecting and containing an incident. I'll meet you there. Welcome back. I want to commend you on such a fantastic job you're doing so far. The skills you are learning will create a solid foundation as you begin your security career. In the previous section, you applied your networking knowledge to deepen your understanding of network traffic. You practiced some skills that security analysts use on the job like capturing network traffic and dissecting packets. Next, we'll examine the lifecycle of a security incident from beginning to end. You'll focus on how to detect, respond, and recover from an incident. Coming up, you'll learn how to investigate and verify an incident once it's been detected. You'll explore the plans and processes behind incident response. Finally, you'll learn about the post-incident actions that organizations take to learn and improve from the experience. At the end of this section, you'll gain a comprehensive understanding of an incident's lifecycle. You're ready? Let's begin. Incidents happen. And as a security analyst, you'll likely be tasked with investigating and responding to security incidents at some point in your career. Let's examine the detection and analysis phase of the incident response lifecycle. This is where incident response teams verify and analyze incidents. Detection enables the prompt discovery of security events. Remember, not all events are incidents, but all incidents are events. Events are regular occurrences in business operations, like visits to a website or password reset requests. IDS and SIEM tools collect and analyze event data from different sources to identify potential unusual activity. If an incident is detected, such as a malicious actor successfully gaining unauthorized access to an account, then an alert is sent out. Security teams then begin the analysis phase. Analysis involves the investigation and validation of alerts. During the analysis process, analysts must apply their critical thinking and incident analysis skills to investigate and validate alerts. They'll examine indicators of compromise to determine if an incident has occurred. This can be a challenge for a couple of reasons. The challenge with detection is, it's impossible to detect everything. Even great detection tools have limitations in how they work. And automated tools may not be fully deployed across an organization due to limited resources. Some incidents are unavoidable, which is why it's important for organizations to have an incident response plan in place. Analysts often receive a high volume of alerts per shift, sometimes even thousands. Most of the time, high alert volumes are caused by misconfigured alert settings. For example, alert rules that are too broad and not tuned to an organization's environment create false positives. Other times, high alert volumes can be legitimate alerts caused by malicious actors taking advantage of a newly discovered vulnerability. As a security analyst, it's important that you're equipped to effectively analyze alerts. And coming up, you'll do just that. You may recall our discussion on the different documentation tools and types used by security teams when responding to incidents. In this video, we'll examine the benefits that documentation offers so that you can better understand how to leverage documentation as a security professional. As a security engineer who has developed a great deal of detection rules, it was critical for me to document what it means when those rules are activated-- what severity to assign, what might lead to false positives, and how the analyst can confirm the alert is legitimate. Without this documentation, a security operations team can never scale beyond one or two analysts. If something was documented, then there's a record of it happening. This means that relevant information can be accessed. This is known as transparency. Transparent documentation is useful as a source of evidence for security insurance claims, regulatory investigations, and legal proceedings. You'll learn more about documentation processes that help to achieve this in an upcoming section. Documentation also provides standardization. This means that there's an established set of guidelines or standards that members of an organization can follow to complete a task or workflow. An example of creating standardization through documentation is establishing an organization's security policy, processes, and procedures. This helps in maintaining quality of work since there are set rules to follow. Documentation also improves clarity. Effective documentation not only gives team members a clear understanding of their roles and duties, but it also provides information on how to get the job done. For example, playbooks that provide detailed instructions prevent uncertainty and confusion during incident response. The security field is constantly changing. Attacks evolve, and regulatory requirements might change. This is why it's important to maintain, review, and update documentation regularly to keep up with any changes. As a security professional, you'll likely juggle documentation responsibilities alongside your other tasks. By taking the time to write down your actions, you'll recall facts and information. You may even notice some gaps in the previous actions you took. The time you spend documenting is valuable, not only for you, but for your entire organization. Let's continue our discussion on how documentation provides transparency through documents like chain of custody. During incident response, evidence must be accounted for during the entire incident's lifecycle. Tracking evidence is important if the evidence is requested as part of any legal proceedings. How can security teams ensure that this is done? They use a form called chain of custody. Chain of custody is the process of documenting evidence, possession, and control during an incident lifecycle. As soon as evidence gets collected, chain of custody forms are introduced. The forms should be filled out with details as the evidence is handled. Let's examine a very simple example of how chain of custody is used during digital forensic analysis. Previously, you learned that digital forensics is the practice of collecting and analyzing data to determine what has happened after an attack. During an incident response, Aisha verified that a compromised hard drive requires examination by the forensics team. First, she ensures that the hard drive is write-protected so the data on the disk can't be edited or erased. Then she calculates and records a cryptographic hash function of an image of the hard drive. Remember that a hash function is an algorithm that produces a code that can't be decrypted. Aisha is then instructed to transfer it to Colin in the forensics department. Colin examines it and sends it off to Nav, another analyst. Nav receives the compromised hard drive and sends it to her manager Arman. Each time the hard drive is transferred to another person, they need to log it in the chain of custody form so that movement of evidence is transparent. Tampering with the data on the hard drive can be detected using the original hash that Aisha documented at the beginning of the process. This ensures that there's a paper trail describing who handled the evidence, and why, when, and where they handled it. Just like other documentation types, there is no standard template of what the chain of custody form should look like. But they do contain common elements. This is what you might examine on a chain of custody log form. First, there should be a description of the evidence, which includes any identifying information, like the location, hostname, MAC address, or IP address. Next is the custody log which details the name of the people who transferred and received the evidence. It also includes the date and time the evidence was collected or transferred and the purpose of the transfer. You may be wondering, what happens if evidence gets logged incorrectly or if there's a missing entry? This is what's known as a broken chain of custody, which occurs when there are inconsistencies in the collection and logging of evidence in the chain of custody. In the court of law, chain of custody documents help establish proof of the integrity, reliability, and accuracy of the evidence. For evidence related to security incidents, chain of custody forms are used to help meet legal standards so that this evidence can be used in legal proceedings. If a malicious actor compromised a system, evidence must be available to determine their actions so that appropriate legal action can be taken. However, in some cases, major breaks in the chain of custody can impact the integrity, reliability, and accuracy of the evidence. This affects whether or not the evidence can be a trusted source of information and used in the court of law. Chain of custody forums provide us with a method of maintaining evidence so that malicious actors can be held responsible for their actions. Have you ever taken a trip to a place you've never visited before? You may have used a travel itinerary to plan your trip activities. Travel itineraries are essential documents to have, especially for travel to a new place. They help keep you organized and give you a clear picture of your travel plans. They detail the activities you'll do, the places you'll visit, and your travel time between destinations. Playbooks are similar to travel itineraries. As you may remember from our previous discussions, a playbook is a manual that provides details about any operational action. They provide security analysts with instructions on exactly what to do when an incident occurs. Playbooks provide security professionals with a clear picture of their tasks during the entire incident response lifecycle. Responding to an incident can be unpredictable and chaotic at times. Security teams are expected to act quickly and effectively. Playbooks offer structure and order during this time by clearly outlining the actions to take when responding to a specific incident. By following a playbook, security teams can reduce any guesswork and uncertainty during response times. This allows security teams to act quickly and without any hesitation. Without playbooks, an effective and swift response to an incident is nearly impossible. Within playbooks, there may be checklists that can also help security teams perform effectively during stressful times by helping them remember to complete each step in the incident response lifecycle. Playbooks outline the steps that are necessary in response to an attack, like ransomware, data breach, malware, or DDoS. Here's an example of a playbook that uses a flow chart diagram with the steps to take during the detection of DDoS attack. This depicts the process for detecting a DDoS and begins with determining the indicators of compromise, like unknown incoming traffic. Once the indicators of compromise are determined, the next step is to collect the logs and, finally, analyze the evidence. There are three different types of playbooks-- non-automated, automated, or semi-automated. The DDoS playbook we just explored is an example of a non-automated playbook, which requires step-by-step actions performed by an analyst. Automated playbooks automate tasks in incident response processes. For example, tasks such as categorizing the severity of the incident or gathering evidence can be done using an automated playbook. Automated playbooks can help lower the time to resolution during an incident. SOAR and SIEM tools can be configured to automate playbooks. Finally, semi-automated playbooks combine a person's action with automation. Tedious, error-prone, or time-consuming tasks can be automated while analysts can prioritize their time with other tasks. Semi-automated playbooks can help increase productivity and decrease time to resolution. As a security team responds to incidents, they may discover that a playbook needs updates or changes. Threats are constantly evolving. And for playbooks to be effective, they must be maintained and updated regularly. A great time to introduce changes to playbooks is during the post-incident activity phase. We'll be exploring more about this phase in an upcoming section. Meet you there. As you've learned, security analysts can be flooded with a large amount of alerts on any given day. How does an analyst manage all of these alerts? Hospital emergency departments receive a large number of patients every day, and each patient needs medical care for a different reason. But not all patients will receive medical care immediately. This is because hospitals have a limited number of resources available and must manage their time and energy efficiently. They do this through a process known as triage. In medicine, triage is used to categorize patients based on the urgency of their conditions. For example, patients with a life threatening condition such as a heart attack will receive immediate medical attention, but a patient with a non-life-threatening condition like a broken finger may have to wait before they see a doctor. Triage helps to manage limited resources so that hospital staff can give immediate attention to patients with the most urgent conditions. Triage is also used in security. Before an alert gets escalated, it goes through a triage process, which prioritizes incidents according to their level of importance or urgency. Similar to hospital emergency departments, security teams have limited resources available to dedicate to incident response. Not all incidents are the same. And some may involve an urgent response. Incidents are triaged according to the threat they pose to the confidentiality, integrity, and availability of systems. For example, an incident involving ransomware requires immediate response. This is because ransomware may cause financial, reputational, and operational damage. Ransomware is a higher priority than an incident like an employee receiving a phishing email. When does triage happen? Once an incident is detected and an alert gets sent out, triage begins. As a security analyst, you'll identify the different types of alerts and then prioritize them according to urgency. The triage process generally looks like this. First, you'll receive and assess the alert to determine if it's a false positive and whether it's related to an existing incident. If it's a true positive, you'll assign priority on the alert based on the organization's policy and guidelines. The priority level defines how the organization's security team will respond to the incident. Finally, you'll investigate the alert and collect and analyze any evidence associated with the alert, such as system logs. As an analyst, you'll want to ensure that you complete a thorough analysis so that you have enough information to make an informed decision about your findings. For example, say that you receive an alert for a failed user login attempt. You'll need to add context to your investigation to determine if it's malicious. You can do so by asking questions. Is there anything out of the ordinary associated with this alert? Are there multiple failed login attempts? Did the login happen outside of normal working hours? Did the login happen outside of the network? These questions paint a picture around the incident. By adding context, you avoid making assumptions, which can result in incomplete or incorrect conclusions. Now that we've covered how to triage alerts, we're ready to discuss how to respond and recover from an incident. Let's go. In this video, we'll discuss the third phase of the incident response lifecycle. This phase includes the steps for how security teams contain, eradicate, and recover from an incident. It's important to note that these steps interrelate. Containment helps meet the goals of eradication, which helps meet the goals of recovery. This phase of the lifecycle also integrates with the core functions of the NIST cybersecurity framework, respond and recover. Let's begin with the first step, containment. After an incident has been detected, it must be contained. Containment is the act of limiting and preventing additional damage caused by an incident. Organizations outline their containment strategies in incident response plans. Containment strategies detail the actions that security teams should take after an incident has been detected. Different containment strategies are used for various incident types. For example, a common containment strategy for a malware incident on a single computer system is to isolate the affected system by disconnecting it from the network. This prevents the spread of the malware to other systems in the network. As a result, the incident is contained to the single compromised system, which limits any further damage. Containment actions are the first step toward removing a threat from an environment. Once an incident has been contained, security teams work to remove all traces of the incident through eradication. Eradication involves the complete removal of the incident elements from all affected systems. For example, eradication actions include performing vulnerability tests and applying patches to vulnerabilities related to the threat. Finally, the last step of this phase in the incident response lifecycle is recovery. Recovery is the process of returning affected systems back to normal operations. An incident can disrupt key business operations and services. During recovery, any services that were impacted by the incident are brought back to normal operation. Recovery actions include reimaging affected systems, resetting passwords, and adjusting network configurations like firewall rules. Remember, the incident response lifecycle is cyclical. Multiple incidents can happen across time. And these incidents can be related. Security teams may have to circle back to other phases in the lifecycle to conduct additional investigations. Next up, we'll discuss the final phase of the lifecycle. Meet you there. Now that a security team has successfully contained, eradicated, and recovered from an incident, their job is done, right? Not quite. Whether it's a new technology or a new vulnerability, there's always more to learn in the security field. The perfect time for learning and improvement happens during the final phase of the incident response lifecycle, post-incident activity. The post-incident activity phase entails the process of reviewing an incident to identify areas for improvement during incident handling. During this phase of the lifecycle, different types of documentation get updated or created. One of the critical forms of documentation that gets created is the final report. The final report is documentation that provides a comprehensive review of an incident. It includes a timeline and details of all events related to the incident and recommendations for future prevention. During an incident, the goal of the security team is to focus efforts on response and recovery. After an incident, security teams work to minimize the risk of it happening again. One way to improve processes is to hold a lessons learned meeting. A lessons learned meeting includes all parties involved in the incident and is generally held within two weeks after the incident. During this meeting, the incident is reviewed to determine what happened, what actions were taken, and how well the actions worked. The final report is also used as the main reference document during this meeting. The goal of the discussions in a lessons learned meeting is to share ideas and information about the incident and how to improve future response efforts. Here are some questions to ask during a lessons learned meeting. What happened? What time did it happen? Who discovered it? How did it get contained? What were the actions taken for recovery? What could have been done differently? Incident reviews can reveal human errors before detection and during response, whether it's a security analyst missing a step in a recovery process or an employee clicking a link in a phishing email resulting in the spread of malware. Blaming someone for an action they did or didn't do should be avoided. Instead, security teams can view this as an opportunity to learn from what happened and improve. That wraps up our discussion on incident investigation and response. Nice work on finishing up another section. We've covered a lot here. So let's take a moment to quickly recap. First, we revisited the detection and analysis phase of the NIST incident response lifecycle and focused on how to investigate and verify an incident. We discussed the purpose of detection and how indicators of compromise can be used to identify malicious activity on a system. Next, we examine plans and processes behind the incident response, such as documentation and triage. We also explored strategies for containing and eradicating an incident and recovering from it. Finally, we examined the last phase of the incident lifecycle, post-incident actions. We talked about final reports, timelines, and the value of scheduling post-incident reviews through lessons learned meetings. As a security analyst, you'll be responsible for completing some processes involved in each phase of the incident response lifecycle. Coming up, you'll learn about logs and have the chance to explore them using a SIEM. History books, receipts, diaries-- what do all these things have in common? They record events. Whether it's historical events, financial transactions, or private diary entries, records preserve event details. And having access to these details can help us in many ways. Previously, we explored the different types of processes and procedures involved during each phase of the incident response lifecycle. In this section, we'll direct our focus on one of the key components of incident investigation, logs and alerts. In security, logs record event details. And these details are used to support investigations. First, you'll learn all about logs, what they are, and how they're created. You'll also learn how to read and analyze logs. Then we'll revisit intrusion detection systems. You'll explore how to interpret signatures. You'll have an opportunity to apply what you've learned through hands-on activities using a tool called Suricata. Finally, you'll search in some tools like Splunk and Chronicle to locate events of interest and access log data. Events are a valuable data source. They help create context around an alert so you can interpret the actions that took place on a system. Knowing how to read, analyze, and connect different events will help you identify malicious behavior and protect systems from attack. Ready? Let's begin. Devices produce data in the form of events. As a refresher, events are observable occurrences that happen on a network, system, or device. This data provides visibility into an environment. Logs are one of the key ways security professionals detect unusual or malicious activity. A log is a record of events that occur within an organization's systems. System activity is recorded in what's known as a log file, or commonly called, logs. Almost every device or system can generate logs. Logs contain multiple entries which detail information about a specific event or occurrence. Logs are useful to security analysts during incident investigation, since they record details of what, where, and when an event occurred on the network. This includes details like date, time, location, the action made, and the names of the users or systems who performed the action. These details offer valuable insight, not only for troubleshooting issues related to system performance, but, most importantly, for security monitoring. Logs allow analysts to build a story and timeline around various event occurrences to understand what exactly happened. This is done through log analysis. Log analysis is the process of examining logs to identify events of interest. Since there are different sources available to get logs, an enormous volume of log data can be generated. It's helpful to be selective in what we log so that we can log efficiently. For example, web applications generate a high volume of log messages, but not all of this data may be relevant to an investigation. In fact, it may even slow things down. Excluding specific data from being logged helps reduce the time spent searching through log data. You may recall our discussion on SIEM technology. SIEM tools provide security professionals with a high-level overview of what happens in a network. SIEM tools do this by first collecting data from multiple data sources. Then the data gets aggregated or centralized in one place. Finally, the diverse log formats get normalized or converted into a single preferred format. SIEM tools help process large log volumes from multiple data sources in real time. This allows security analysts to quickly search for log data and perform log analysis to support their investigations. So how do logs get collected? Software known as log forwarders collect logs from various sources and automatically forward them to a centralized log repository for storage. Since different types of devices and systems can create logs, there are different log data sources in an environment. These include network logs, which are generated by devices such as proxies, routers, switches, and firewalls, and system logs, which are generated by operating systems. There's also application logs, which are logs related to software applications, security logs, which are generated by security tools like IDS or IPS and, lastly, authentication logs which record login attempts. Here's an example of a network log from a router. There are a couple of log entries here, but we'll focus on the first line. Here we can observe a number of fields. First, there's an action specifying, ALLOW. This means that the router's firewall settings allowed access from a specific IP address to google.com. Next there's a field specifying the source, which lists an IP address. So far, the information from this log entry is telling us that network traffic to google.com from this source IP address is allowed. The last field specifies the timestamp which is one of the most essential fields in a log. We can identify the exact date and time of an action that's occurred. This is useful for correlating multiple events to develop a timeline of the incident. There you have it. You've analyzed your first network log. Coming up, we'll continue our discussion on logs and explore log formats. When you purchase an item in a store, you usually receive a receipt as a record of purchase. The receipt breaks down the transaction information with details, such as the date and time, the cashier's name, the item name, cost, and the method of payment. But not all store receipts look the same. For example, receipts like automotive invoices use lots of detail when listing the items or services that were sold. You most likely won't find this much detail from a restaurant receipt. Despite the differences among store receipts, all receipts contain important details that are relevant to the transaction. Logs are similar to receipts. While receipts record purchases, logs record the events or activities that happen on a network or system. As a security analyst, you'll be responsible for interpreting logs. Logs come in different formats, so not all logs look the same. But they usually contain information like timestamps, system characteristics like IP addresses, and a description of the event, including the action taken and who performed the action. We know that logs can be generated from many different data sources, such as network devices, operating systems, and more. These log sources generate logs in different formats. Some log formats are designed to be human readable, while others are machine readable. Some logs can be verbose, which means they contain lots of information, while some are short and simple. Let's explore some commonly used log formats. One of the most commonly used log formats is syslog. Syslog is both a protocol and a log format. As a protocol, it transports and writes logs. As a log format, it contains a header followed by structured data and a message. The syslog entry includes three sections, a header, structure data, and a message. The header contains data fields like timestamp, the hostname, the application name, and the message ID. The structured data portion contains additional data information and key value pairs. Here, the eventSource is a key that specifies the data source of the log, which is the value, Application. Lastly, the message component contains the detailed log message about the event. In this example, "This is a log entry." is the message. Let's explore another common log format you might encounter as a security analyst. JavaScript Object Notation, more popularly known as JSON, is a text-based format designed to be easy to read and write. It also uses key value pairs to structure data. Here's an example of a JSON log. The curly brackets represent the beginning and end of an object. The object is the data that's enclosed between the brackets. It's organized using key value pairs where each key has a corresponding value separated by colons. For example, for the first line, the key is Alert, and the value is Malware. JSON is known for its simplicity and easy readability. As a security analyst, you'll use JSON to read and write data-like logs. Extensible Markup Language, or XML, is a language and a format used for storing and transmitting data. Instead of key value pairs, it uses tags and other keys to structure data. Here we have an example of an XML log entry with four fields-- firstName, lastName, employeeId, and dateJoined, which are separated with arrows. Finally, Comma-Separated Values, or CSV, is a format that uses separators like commas to separate data values. In this example, there are many different data fields which are separated with commas. Now that you know about the diversity of log formats, you can focus on evaluating logs to build context around a detection. Coming up, you'll explore how IDS signatures are used to detect, log, and alert on suspicious activity. Detection requires data. And this data can come from various data sources. You've already explored how different devices produce logs. Now we'll examine how different detection technologies monitor devices and log different types of system activity like network and endpoint telemetry. Telemetry is the collection and transmission of data for analysis. While logs record events occurring on systems, telemetry describes the data itself. For example, packet captures are considered network telemetry. For security professionals, logs and telemetry are sources of evidence that can be used to answer questions during investigations. Previously, you learned about an intrusion detection system, or IDS. Remember that IDS is an application that monitors activity and alerts on possible intrusions. This includes monitoring different parts of a system or network like an endpoint. An endpoint is any device connected on a network, such as a laptop, tablet, desktop computer, or a smartphone. Endpoints are entry points into a network which makes them key targets for malicious actors looking to gain unauthorized access into a system. To monitor endpoints for threats or attacks, a host-based intrusion detection system can be used. It's an application that monitors the activity of the host on which it's installed. To clarify, a host is any device that communicates with other devices on a network, similar to an endpoint. Host-based intrusion detection systems are installed as an agent on a single host, such as a laptop computer or a server. Depending on its configuration, host-based intrusion detection systems will monitor the host on which it's installed to detect suspicious activity. Once something's been detected, it records output as logs, and an alert gets generated. What if we wanted to monitor a network? A network-based intrusion detection system collects and analyzes network traffic and network data. Network-based intrusion detection systems work similar to packet sniffers because they analyze network traffic and network data on a specific point in the network. It's common to deploy multiple IDS sensors at different points in the network to achieve adequate visibility. When suspicious or unusual network activity is detected, the network-based intrusion detection system logs it and generates an alert. In this example, the network-based intrusion detection system is monitoring the traffic that's both coming from and going to the internet. Intrusion detection systems use different types of detection methods. One of the most common methods is signature analysis. Signature analysis is a detection method used to find events of interest. A signature specifies a set of rules that an IDS refers to when it monitors activity. If the activity matches the rules in the signature, the IDS logs it and sends out an alert. For example, a signature can be written to generate an alert if a failed login on a system happens three times in a row, which suggests a possible password attack. Before alerts are generated, the activity must be logged. IDS technologies record the information of the devices, systems, and networks which they monitor as IDS logs. IDS logs can then be sent, stored, and analyzed in a centralized log repository like a SIEM. Coming up, we'll explore how to read and configure signatures. We'll meet you there. As a security analyst, you may be tasked with writing, customizing, or testing signatures. To do this, you'll use IDS tools. So in this section, we'll examine signature syntax. And by the end, you'll be able to read a signature. A signature specifies detection rules. These rules outline the types of network intrusions you want an IDS to detect. For example, a signature can be written to detect an alert on suspicious traffic attempting to connect to a port. Rule language differs depending on different network intrusion detection systems. The term, Network Intrusion Detection System, is often abbreviated as the acronym, NIDS, and pronounced, "nids." Generally, NIDS rules consist of three components-- an action, a header, and rule options. Now let's examine each of these three components in more detail. Typically, the action is the first item specified in the signature. This determines the action to take if the rule criteria matches are met. Actions differ across NIDS rule languages. But some common actions are alert, pass, or reject. Using our example, if a rule specifies to alert on suspicious network traffic that establishes an unusual connection to a port, the IDS will inspect the traffic packets and send out an alert. The header defines the signature's network traffic. These include information such as source and destination IP addresses, source and destination ports, protocols, and traffic direction. If we want to detect and alert on suspicious traffic connecting to a port, we have to first define the source of the suspicious traffic in the header. Suspicious traffic can originate from IP addresses outside the local network. It can also use specific or unusual protocols. We can specify external IP addresses and these protocols in the header. Here's an example of how header information may appear in a basic rule. First, we can observe that the protocol, TCP, is the first listed item in the signature. Next, the sauce IP address, 10.120.170.17, and the source port number is specified as being any. The arrow in the middle of the signature indicates the direction of the network traffic. So we know it's originating from the source IP 10.120.170.17 from any port going to the following destination IP address, 133.113.202.181 and destination port 80. The rule options let you customize signatures with additional parameters. There are many different options available to use. For instance, you can set options to match the content of a network packet to detect malicious payloads. Malicious payloads reside in a packet's data and perform malicious activity like deleting or encrypting data. Configuring rule options helps in narrowing down network traffic so you can find exactly what you're looking for. Typically, rule options are separated by semicolons and enclosed in parentheses. In this example, we can examine that the rule options are enclosed in a pair of parentheses and are also separated with semicolons. The first rule option, msg, which stands for message, provides the alert's text. In this case, the alert will print out the text, "This is a message." There's also the option sid, which stands for signature ID. This attaches a unique ID to each signature. The rev option stands for revision. Each time a signature is updated or changed, the revision number changes. Here, the number 1 means it's the first version of the signature. Great. Now you've developed another skill in your journey towards becoming a security analyst, how to read signatures. There's so much more to learn. And coming up, we'll discuss tools that use signatures. Previously, you learned about signature-based analysis. You also learned how to read signatures used in network-based intrusion detection systems. Here, we'll use an open-source signature-based IDS called Suricata to examine a signature. Many NIDS technologies come with pre-written signatures. And you can think of these signatures as customizable templates, sort of different templates available in a word processor. These signature templates provide you with a starting point for writing and defining your rules. You can also write and add your own rules. Let's examine a pre-written signature through Suricata. On this Linux machine running Ubuntu, Suricata is already installed. Let's examine some of its files by changing directories to the /etc/ directory and into the /suricata directory. This is where all of Suricata's configuration files live. Next, we'll use the ls command to list the contents of the /suricata directory. There's a couple of different files in here, but we'll focus on the rules folder. This is where the pre-written signatures are. You can also add custom signatures here. We'll use the cd command followed by the name of the folder to navigate to that folder. Using the ls command, we can observe that the folder contains some rule templates for different protocols and services. Let's examine the custom.rules file using the less command. As a quick refresher, the less command returns the content of a file one page at a time, which makes it easy to move forward and backward through the content. We'll use the arrow key to scroll up. Lines that begin with a pound sign are comments meant to provide context for those who read them and are ignored by Suricata. The first line says, "Custom rules example for http connection." This tells us that this file contains custom rules for HTTP connections. We can observe that there's a signature. The first word specifies the signature's action. For this signature, the action is alert. This means that the signature generates an alert when all of the conditions are met. The next part of the signature is the header. It specifies the protocol, http. The source IP address is $HOME_NET, and the source port is defined as any. The arrow indicates the direction of traffic coming from the home network and going to the destination IP address, $EXTERNAL_NET and any destination port. So far, we know that this signature triggers an alert when it detects any HTTP traffic leaving the home network and going to the external network. Let's examine the remainder of the signature to identify if there's any additional conditions the signature looks for. The last part of the signature includes the rule options. They're enclosed in parentheses and separated by semicolons. There's many options listed here, but we'll focus on the message, flow, and content options. The message option will show the message, "GET on wire" once the alert is triggered. The flow option is used to match on direction of network traffic flow. Here it's established. This means that a connection has been successfully made. The content option inspects the content of a packet. Here, between the quotation marks, the text, "GET" is specified. GET is an HTTP request that's used to retrieve and request data from a server. This means the signature will match if a network packet contains the text, get, indicating a request. To summarize, this signature alerts any time Suricata observes the text get in an HTTP connection from the home network going to the external network. Every environment is different. And in order for an IDS to be effective, signatures must be tested and tailored. As a security analyst, you may test, modify, or create IDS signatures to improve the detection of threats in an environment and reduce the likelihood of false positives. Coming up, we'll examine how Suricata logs events. Meet you there. Now let's examine some logs generated by Suricata. In Suricata, alerts and events are output in a format known as EVE JSON. EVE stands for Extensible Event Format, and JSON stands for JavaScript Object Notation. As you previously learned, JSON uses key value pairs, which simplifies both searching and extracting text from log files. Suricata generates two types of log data, alert logs and network telemetry logs. Alert logs contain information that's relevant to security investigations. Usually, this is the output of signatures which have triggered an alert. For example, a signature that detects suspicious traffic across the network generates an alert log that captures details of that traffic. While network telemetry logs contain information about network traffic flows, network telemetry is not always security-relevant. It's simply recording what's happening on a network, such as a connection being made to a specific port. Both of these log types provide information to build a story during an investigation. Let's examine an example of both log types. Here's an example of an event log. We can tell that this event is an alert because the event_type field says alert. There's also details about the activity that was logged including IP addresses and the protocol. There are also details about the signature itself, such as the message and ID. From the signatures message, it appears that this alert relates to the detection of malware. Next up, we have an example of a network telemetry log which shows us the details of an HTTP request to a website. The event_type field tells us it's an HTTP log. There's details about the request. Under hostname, there's the website that was accessed. The user_agent is the name of software that connects you to the website. In this case, it's the web browser, Mozilla/5.0 and the content type, which is the data the HTTP request returned. Here it's specified as HTML text. That sums it up on the different types of log outputs. In the upcoming activity, you'll be applying what we just explored by getting hands-on with Suricata. Have fun. As a security analyst, you'll need to be able to quickly access the relevant data required to perform your duties. Whether it's triaging alerts, monitoring systems, or analyzing log data during incident investigations, a SIEM is the tool for this job. As a quick review, a SIEM is an application that collects and analyzes log data to monitor critical activities in an organization. It does this by collecting, analyzing, and reporting on security data from multiple sources. Previously, you learned about the SIEM process for data collection. Let's revisit this process. First, SIEM tools collect and process enormous amounts of data generated by devices and systems from all over an environment. Not all data is the same. As you already know, devices generate data in different formats. This can be challenging because there is no unified format to represent the data. SIEM tools make it easy for security analysts to read and analyze data by normalizing it. Raw data gets processed so that it's formatted consistently, and only relevant event information is included. Finally, SIEM tools index the data so it can be accessed through search. All of the events across all the different sources can be accessed with your fingertips. Isn't that useful? SIEM tools make it easy to quickly access and analyze the data flows happening across networks in an environment. As a security analyst, you may encounter different SIEM tools. It's important that you're able to adjust and adapt to whichever tool your organization ends up using. With that in mind, let's explore some SIEM tools currently used in the security industry. Splunk is a data analysis platform. Splunk Enterprise Security provides SIEM solutions that let you search, analyze, and visualize security data. First, it collects data from different sources. That data gets processed and stored in an index. Then it can be accessed in a variety of different ways, like through search. Chronicle is Google Cloud's SIEM which stores security data for search, analysis, and visualization. First, data gets forwarded to Chronicle. This data then gets normalized or cleaned up so it's easier to process and index. Finally, the data becomes available to be accessed through a search bar. Next up, we'll explore how to search on these SIEM platforms. Now that we've reviewed how a SIEM works, let's learn how to search and query events in a SIEM database. Data that's been imported into a SIEM can be accessed by entering queries into the SIEM's search engine. Massive amounts of data can be stored in a SIEM database. Some of this data may date back years. This can make searching for security events challenging. For example, let's say you're searching to find a failed login event. You search for the event using the keywords, failed login. This is a very broad query which can return thousands of results. Broad search queries like this slow down the response times of a search engine, since it's searching across all the index data. But if you specify additional parameters like an event ID and a date and time range, you can narrow down the search to get faster results. It's important that search queries are specific so that you can find exactly what you're looking for and save time in the search process. Different SIEM tools use different search methods. For example, Splunk uses its own query language called Search Processing Language, or SPL, for short. SPL has many different search options you can use to optimize search results so that you can get the data you're looking for. For now, I'll demonstrate a raw log search in Splunk Cloud for events that reference errors or failures for a fictional online store called Buttercup Games. First, we'll use the search bar to type in our query, Buttercupgames error OR fail*. This search is specifying the index, which is Buttercupgames. We also specify the search terms, error OR fail. The Boolean operator, OR, ensures that both of the keywords will be searched. The asterisk at the end of the term, fail*, is known as a wildcard. This means it will search for all possible endings that contain the term fail. This helps us expand our search results because events may label failures differently. For example, some events may use the term failed. Next, we'll select a time range using the time range picker. Remember, the more specific our search is, the better. Let's search for data from the last 30 days. Under the Search bar, we have our search results. There's a timeline, which gives us a visual representation of the number of events over a period. This can be helpful in identifying event patterns, such as peaks in activity. Under the Timeline, there's the Events Viewer which gives us a list of events that match our search. Notice how our search terms, Buttercupgames and error are highlighted in each event. It doesn't appear that any events matching with the term fail were found. Each event has a timestamp and raw log data. For the events with errors, it appears that there's an error relating to the HTTP cookies used in the Buttercupgames website. At the bottom of the raw log data, there's some information related to the data source, including the hostname, source, and sourcetype. This information tells us where the event data originated from, such as a device or file. If we click on it, we can choose to exclude it from the search results. On the Search bar, we can examine that the search terms have been changed, and host not equal www1 has been added, which means not to include www1 hosts. Notice that the new search results do not contain www1 as a host, but contain www2 and www3. This is just one of the many ways that you can target your searches to retrieve information you're looking for. This search is known as a raw log search, which has a slower search performance since it extracts log data fields during the search process. As a security analyst, you'll use different commands to optimize search performance for faster search results. That completes querying in Splunk. You've learned the importance of effective queries and how to perform a basic Splunk search. Coming up, you'll learn how to query events in Chronicle. Chronicle allows you to search and filter log data. In this video, we'll explore using Chronicle's search field to locate an event. Chronicle uses the YARA-L language to define rules for detection. It's a computer language used to create rules for searching through ingested log data. For example, you can use YARA-L to write a rule to detect specific activities related to the exfiltration of valuable data. Using Chronicle's Search field, you can search for fields like hostname, domain, IP, URL, email, username, or file hash. Using the Search field, you can enter different types of searches. The default method of search is using UDM search, which stands for Unified Data Model. It searches through normalized data. If you can't find the data you're looking for searching the normalized data, you have the option of searching raw logs. Raw log search searches through the logs which have not been normalized. From our earlier discussion on the SIEM process, you may recall that raw logs get processed during the normalization step. During normalization, all of the relevant information from raw logs gets extracted and formatted, making the data easier to search. A reason we might need to search raw logs is to find data that may not have been included in the normalized logs, like specific fields which have not been normalized or to troubleshoot data ingestion problems. Let's examine a UDM search for a failed login using Chronicle. First, let's click on the Structured Query Builder icon so that we can perform a UDM search. I'll type in the search metadata.event_type="USER_LOGIN" AND security_result.action=BLOCK. Let's break down this UDM search. Since we are searching for normalized data, we need to specify a search that uses UDM format. UDM events have a set of common fields. The metadata.event_type field details the event's type. Here, we're asking Chronicle to find an authentication activity event of USER_LOGIN. Next, there's AND, which is a logical operator that tells the search engine to contain both terms. Finally, the security_result.action field specifies a security action such as ALLOW or BLOCK. Here, the action is BLOCK. This means the user login was blocked or failed. Now we'll press the query button. We're going to focus on searching normalized data. We're presented with a screen with the search results. There's lots of information here. Under UDM Search, we can observe our search terms. There's also a bar graph timeline visualizing the failed login events over a period. At a quick glance, this gives us a snapshot of the failed login activity over time, allowing us to spot possible patterns. Under the Timeline, there's a list of events with timestamps associated with this search. Under each event, there's an asset, which is the name of a device. For example, this event shows a failed login for a username, alice. If we click the event, we can open up the raw log associated with the event. We can interpret these raw logs for more detail about the event's activity during the investigation. To the left, there's Quick Filters. These are additional fields or values that we can use to filter the search results. For example, if we click target.ip, we are given a list of IP addresses. If we click one of these IP addresses, we can filter the search results to contain only this target IP address. This helps us find specific data we're looking for and helps us save time in the process. Great work. Now you know how to perform a search using Chronicle. In the upcoming activity, you'll have the chance to perform searches using the SIEM tools we've just discussed. Congratulations. You made it to the end of this section. You've made so much progress in your security journey. Let's review what we learned. You learned all about how to read and analyze logs. You examined how log files are created and used for analysis. You also compare different types of common log formats and learned how to read them. You extended your understanding on intrusion detection systems by comparing network-based systems and host-based systems. You also learned how to interpret signatures. You examined how signatures are written in also how to detect, log, and alert on intrusions. You interacted with Suricata in the command line to examine and interpret signatures and alerts. Lastly, you learned how to search in SIEM tools like Splunk and Chronicle. You learned about the importance of crafting tailored queries to locate events. At the forefront of incident response, monitoring and analyzing network traffic for indicators of compromise is one of the primary goals. Being able to perform in-depth log analysis and knowing how to read and write signatures and how to access log data are all skills that you'll use as a security analyst. Congratulations on completing this course on Detection and Response. As you've progressed, we've covered a wide range of topics and tools. Let's take a moment to review what you've learned. First, we began with an overview of the incident response lifecycle. You learned how security teams coordinate their response efforts, and you explored the documentation, detection, and management tools used in incident response. Next, you learned how to monitor and analyze network traffic. You learned about capturing and analyzing packets using packet sniffers. You also practiced using tools like tcpdump to capture and analyze network data to identify indicators of compromise. Then we explored processes and procedures involved in the phases of the incident response lifecycle. You learned about techniques related to incident detection and analysis. You also learned about documentation like chain of custody, playbooks, and final reports. We ended with exploring strategies used for recovery and post-incident activity. Finally, you learned how to interpret logs and alerts. You explored Suricata on the command line to read and understand signatures and rules. You also used SIEM tools like Splunk and Chronicle to search for events and logs. As a security analyst, you'll be presented with a new challenge every day. Whether it's investigating evidence or documenting your work, you'll use what you've learned in this course to effectively respond to incidents. I'm so glad to have been on this learning journey with you. You've done a fantastic job in expanding your knowledge and learning new tools to add to your security toolbox. One of the things I love about the security field is that there's always something new to learn. And coming up, you'll continue your learning journey by exploring a programming language called Python, which can be used to automate security tasks. Keep up the great work. [MUSIC PLAYING] 